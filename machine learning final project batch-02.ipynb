{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import math\n",
    "import matplotlib.mlab as mlab\n",
    "from pandas.io.json import json_normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "\n",
    "import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "\n",
    "import shap\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Activation, LeakyReLU\n",
    "from keras import optimizers, activations, models\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, LearningRateScheduler, LambdaCallback, Callback\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import model_selection, preprocessing, metrics\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = 999\n",
    "Flattening the JSON blobs\n",
    "\n",
    "def load_df(csv_path='data/train.csv', nrows=None):\n",
    "    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n",
    "\n",
    "    df = pd.read_csv(csv_path,\n",
    "                     converters={column: json.loads for column in JSON_COLUMNS},\n",
    "                     dtype={'fullVisitorId': 'str'}, # Important!!\n",
    "                     nrows=nrows)\n",
    "\n",
    "    for column in JSON_COLUMNS:\n",
    "        column_as_df = json_normalize(df[column])\n",
    "        column_as_df.columns = [\"{subcolumn}\" for subcolumn in column_as_df.columns]\n",
    "        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n",
    "    print(\"Loaded {os.path.basename(csv_path)}. Shape: {df.shape}\")\n",
    "    return df\n",
    "%%time\n",
    "train_df = load_df()\n",
    "%%time\n",
    "test_df = load_df(\"/data/test.csv\")\n",
    "train_df.columns.difference(test_df.columns)\n",
    "We can see now that:\n",
    "\n",
    "train.csv has 903,653 rows and 55 columns\n",
    "test.csv has 804,684 rows and 53 columns\n",
    "The columns not included in test.csv are totals.transactionRevenue as expected and trafficSource.campaignCode\n",
    "\n",
    "2. Data Exploration\n",
    "Now the dataset has been flattened, it is easy to refer to any part of the dataframe and explore various features. Firstly, let's look at the target variable totals.transactionRevenue for each user.\n",
    "\n",
    "train_df.head()\n",
    "test_df.head()\n",
    "There also appear to be many columns with constant information in, such as NaN or not available in demo dataset. If entire columns contain the same information like these then they will likely not be of use. The code below tests for columns that only have 1 unique value.\n",
    "\n",
    "const_cols = [c for c in train_df.columns if train_df[c].nunique(dropna=False)==1 ]\n",
    "const_cols\n",
    "train_df = train_df.drop(const_cols, axis=1)\n",
    "train_df.shape\n",
    "test_df = test_df.drop(const_cols, axis=1)\n",
    "test_df.shape\n",
    "train_df.to_csv(\"data/newTrain.csv\")\n",
    "test_df.to_csv(\"data/newTest.csv\")\n",
    "The columns that contain the null or useless values have been removed from the datasets, which now have 19 fewer columns. I have also saved the new dataframes to new csv files as they will be easier to acquire and work from later.\n",
    "\n",
    "nTrain_df = pd.read_csv(\"data/newTrain.csv\",\n",
    "                 dtype={'fullVisitorId': 'str',\n",
    "                       'campaignCode': 'str'})\n",
    "print (nTrain_df.shape)\n",
    "nTrain_df.head()\n",
    "\n",
    "nTest_df = pd.read_csv(\"data/newTest.csv\",\n",
    "             dtype={'fullVisitorId': 'str'})\n",
    "print (nTest_df.shape)\n",
    "nTest_df.head()\n",
    "nTrain_df[\"transactionRevenue\"].fillna(0, inplace=True)\n",
    "nTrain_df[\"transactionRevenue\"] = nTrain_df[\"transactionRevenue\"].astype('float')\n",
    "gdf = nTrain_df.groupby(\"fullVisitorId\")[\"transactionRevenue\"].sum().reset_index()\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(range(gdf.shape[0]), np.sort(np.log1p(gdf[\"transactionRevenue\"].values)))\n",
    "plt.xlabel('index', fontsize=12)\n",
    "plt.ylabel('TransactionRevenue', fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "It is clear to see the revenue comes from a select few of customers, as stated by the 80/20 rule\n",
    "\n",
    "num_buyers = (gdf[\"transactionRevenue\"]>0).sum()\n",
    "print (num_buyers)\n",
    "print(\"Number of unique customers: \",gdf.shape[0],\" with non-zero revenue : \", num_buyers, \"and the ratio is : \", (num_buyers / gdf.shape[0])*100)\n",
    "print (\"The ratio is much lower than 80/20, only 1.3% of visitors have spent money\")\n",
    "\n",
    "\n",
    "minimum = np.log1p(np.min(gdf[\"transactionRevenue\"]))\n",
    "maximum = np.log1p(np.max(gdf[\"transactionRevenue\"]))\n",
    "mean= np.log1p(np.mean(gdf[\"transactionRevenue\"]))\n",
    "std= np.log1p(np.std(gdf[\"transactionRevenue\"]))\n",
    "\n",
    "# Show the calculated statistics\n",
    "print(\"Statistics for total revenue per fullVisitorID:\\n\")\n",
    "print(\"Minimum: ${}\".format(minimum))\n",
    "print(\"Maximum: ${}\".format(maximum))\n",
    "print(\"Mean: ${}\".format(mean))\n",
    "print(\"Standard deviation: ${}\".format(std))\n",
    "Statistics for total revenue per fullVisitorID:\n",
    "\n",
    "\n",
    "sigma = math.sqrt(std)\n",
    "x = np.linspace(mean - 3*sigma, mean + 3*sigma, 100)\n",
    "plt.plot(x,mlab.normpdf(x, mean, sigma))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def horizontal_bar_chart(y1, x1, y2, x2, y3, x3, data):\n",
    "    f, axes = plt.subplots(1,3,figsize=(18, 6))\n",
    "\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    sns.set_color_codes(\"pastel\")\n",
    "    sns.barplot(y=y1, x=x1, data=data,\n",
    "            label=\"Total\", ax=axes[0])\n",
    "\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    sns.set_color_codes(\"pastel\")\n",
    "    sns.barplot(y=y2, x=x2, data=data,\n",
    "            label=\"Total\", ax=axes[1])\n",
    "\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    sns.set_color_codes(\"pastel\")\n",
    "    sns.barplot(y=y3, x=x3, data=data,\n",
    "            label=\"Total\", ax=axes[2])\n",
    "\n",
    "    plt.tight_layout()\n",
    "data = nTrain_df.groupby(\"browser\")[\"transactionRevenue\"].agg(['size', 'count', 'mean'])\n",
    "data.columns = [\"count\", \"count of transactions\", \"mean transaction\"]\n",
    "data.reset_index(inplace=True)\n",
    "data = data.sort_values(by=\"count\", ascending=False)\n",
    "\n",
    "horizontal_bar_chart(\"browser\", \"count\", \"browser\", \"count of transactions\", \"browser\", \"mean transaction\", data.head(10))\n",
    "\n",
    "data = nTrain_df.groupby(\"deviceCategory\")[\"transactionRevenue\"].agg(['size', 'count', 'mean'])\n",
    "data.columns = [\"count\", \"count of transactions\", \"mean transaction\"]\n",
    "data.reset_index(inplace=True)\n",
    "data = data.sort_values(by=\"count\", ascending=False)\n",
    "\n",
    "horizontal_bar_chart(\"deviceCategory\", \"count\", \"deviceCategory\", \"count of transactions\", \"deviceCategory\", \"mean transaction\", data)\n",
    "\n",
    "data = nTrain_df.groupby(\"operatingSystem\")[\"transactionRevenue\"].agg(['size', 'count', 'mean'])\n",
    "data.columns = [\"count\", \"count of transactions\", \"mean transaction\"]\n",
    "data.reset_index(inplace=True)\n",
    "data = data.sort_values(by=\"count\", ascending=False)\n",
    "\n",
    "horizontal_bar_chart(\"operatingSystem\", \"count\", \"operatingSystem\", \"count of transactions\", \"operatingSystem\", \"mean transaction\", data)\n",
    "\n",
    "data = nTrain_df.groupby(\"continent\")[\"transactionRevenue\"].agg(['size', 'count', 'mean'])\n",
    "data.columns = [\"count\", \"count of transactions\", \"mean transaction\"]\n",
    "data.reset_index(inplace=True)\n",
    "data = data.sort_values(by=\"count\", ascending=False)\n",
    "\n",
    "horizontal_bar_chart(\"continent\", \"count\", \"continent\", \"count of transactions\", \"continent\", \"mean transaction\", data)\n",
    "\n",
    "data = nTrain_df.groupby(\"subContinent\")[\"transactionRevenue\"].agg(['size', 'count', 'mean'])\n",
    "data.columns = [\"count\", \"count of transactions\", \"mean transaction\"]\n",
    "data.reset_index(inplace=True)\n",
    "data = data.sort_values(by=\"count\", ascending=False)\n",
    "\n",
    "horizontal_bar_chart(\"subContinent\", \"count\", \"subContinent\", \"count of transactions\", \"subContinent\", \"mean transaction\", data)\n",
    "\n",
    "data = nTrain_df.groupby(\"country\")[\"transactionRevenue\"].agg(['size', 'count', 'mean'])\n",
    "data.columns = [\"count\", \"count of transactions\", \"mean transaction\"]\n",
    "data.reset_index(inplace=True)\n",
    "data = data.sort_values(by=\"count\", ascending=False)\n",
    "\n",
    "horizontal_bar_chart(\"country\", \"count\", \"country\", \"count of transactions\", \"country\", \"mean transaction\", data.head(10))\n",
    "\n",
    "data = nTrain_df.groupby(\"region\")[\"transactionRevenue\"].agg(['size', 'count', 'mean'])\n",
    "data.columns = [\"count\", \"count of transactions\", \"mean transaction\"]\n",
    "data.reset_index(inplace=True)\n",
    "data = data.sort_values(by=\"count\", ascending=False)\n",
    "\n",
    "horizontal_bar_chart(\"region\", \"count\", \"region\", \"count of transactions\", \"region\", \"mean transaction\", data.head(10))\n",
    "\n",
    "data = nTrain_df.groupby(\"source\")[\"transactionRevenue\"].agg(['size', 'count', 'mean'])\n",
    "data.columns = [\"count\", \"count of transactions\", \"mean transaction\"]\n",
    "data.reset_index(inplace=True)\n",
    "data = data.sort_values(by=\"count\", ascending=False)\n",
    "\n",
    "horizontal_bar_chart(\"source\", \"count\", \"source\", \"count of transactions\", \"source\", \"mean transaction\", data.head(10))\n",
    "\n",
    " \n",
    "nTrain_df['datetime'] = nTrain_df['date'].apply(lambda x: datetime.date(int(str(x)[:4]), int(str(x)[4:6]), int(str(x)[6:])))\n",
    "nTest_df['datetime'] = nTest_df['date'].apply(lambda x: datetime.date(int(str(x)[:4]), int(str(x)[4:6]), int(str(x)[6:])))\n",
    "def line_plot( data):\n",
    "    f, axes = plt.subplots(figsize=(18, 6))\n",
    "\n",
    "    sns.set(style=\"white\")\n",
    "    sns.lineplot( data=data, palette=\"tab10\", linewidth=2.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "data = nTrain_df.groupby(\"datetime\")[\"transactionRevenue\"].agg(['count'])\n",
    "line_plot(data)\n",
    "\n",
    "\n",
    "data = nTrain_df.groupby(\"datetime\")[\"transactionRevenue\"].agg(['size'])\n",
    "line_plot(data)\n",
    "\n",
    "\n",
    "data = nTest_df.groupby(\"datetime\")[\"fullVisitorId\"].agg(['size'])\n",
    "line_plot(data)\n",
    "\n",
    "\n",
    "# sort by date for train-validation split\n",
    "nTrain_df.sort_values(by=[\"datetime\"], inplace=True)\n",
    "\n",
    "# extract sorted dates\n",
    "dates = pd.DataFrame(nTrain_df[\"date\"])\n",
    "\n",
    "# set target\n",
    "target = pd.DataFrame(nTrain_df[\"transactionRevenue\"])\n",
    "\n",
    "# extract test IDs for later\n",
    "test_ids = nTest_df[\"fullVisitorId\"].values\n",
    "\n",
    "# columns to be dropped before training the model\n",
    "reduce_columns = [\"bounces\", \"Unnamed: 0\", \"fullVisitorId\", \"date\", \"sessionId\", \"visitId\", \"visitStartTime\", \"isMobile\", \"city\",\n",
    "                 \"metro\", \"networkDomain\", \"region\", \"adContent\", \"adwordsClickInfo.adNetworkType\",\n",
    "                  \"adwordsClickInfo.gclId\", \"adwordsClickInfo.isVideoAd\", \"adwordsClickInfo.page\",\n",
    "                  \"adwordsClickInfo.slot\", \"campaign\", \"isTrueDirect\", \"keyword\",\n",
    "                  \"medium\", \"referralPath\", \"source\", \"datetime\"]\n",
    "\n",
    "# drop columns from both datasets, including the two that are missing in the test dataset\n",
    "train_features = nTrain_df.drop(reduce_columns + [\"campaignCode\", \"transactionRevenue\"], axis=1)\n",
    "\n",
    "test = nTest_df.drop(reduce_columns, axis=1)\n",
    "target.head()\n",
    "\n",
    "# One-hot encode data\n",
    "categorical_features = [\"channelGrouping\", \"browser\", \"deviceCategory\", \"operatingSystem\",\n",
    "                        \"continent\", \"country\", \"subContinent\"]\n",
    "train_features = pd.get_dummies(train_features, columns=categorical_features)\n",
    "\n",
    "test = pd.get_dummies(test, columns=categorical_features)\n",
    "\n",
    "# Since the train and test datasets have different number of categories after one-hot encoding they need to be aligned\n",
    "# an outer join includes all categories from both datasets\n",
    "train_features, test = train_features.align(test, join='outer', axis=1)\n",
    "# Replace NaN values throughout dataset\n",
    "train_features.replace(to_replace=np.nan, value=0, inplace=True)\n",
    "train_features.shape\n",
    "(903653, 425)\n",
    "# Replace NaN values throughout dataset\n",
    "test.replace(to_replace=np.nan, value=0, inplace=True)\n",
    "test.shape\n",
    "(804684, 425)\n",
    "# Log transform the labels in the target\n",
    "target = np.log1p(target)\n",
    "target.head()\n",
    "\n",
    "# Normalise data\n",
    "normalized_features = [\"visitNumber\",  \"hits\", \"newVisits\", \"pageviews\"]\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "train_features[normalized_features] = scaler.fit_transform(train_features[normalized_features])\n",
    "\n",
    "test[normalized_features] = scaler.fit_transform(test[normalized_features])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def k_fold_split(X, y, dates, num_folds):\n",
    "\n",
    "    k = int(np.floor(float(X.shape[0]) / num_folds))\n",
    "\n",
    "    X_train_list = []\n",
    "    y_train_list = []\n",
    "    X_val_list = []\n",
    "    y_val_list = []\n",
    "    dates_list = []\n",
    "\n",
    "    for i in range(2, num_folds + 1):\n",
    "\n",
    "        # percentage split\n",
    "        split = float(i-1)/i\n",
    "\n",
    "        x_temp = X[:k*i]\n",
    "        y_temp = y[:k*i]\n",
    "        dates_list.append((dates.iloc[k*i]).values)\n",
    "\n",
    "        # index to split current fold into train and test\n",
    "        index = int(np.floor(x_temp.shape[0] * split))\n",
    "\n",
    "        X_train = x_temp[:index]\n",
    "        y_train = y_temp[:index]\n",
    "\n",
    "        X_val = x_temp[index:]\n",
    "        y_val = y_temp[index:]\n",
    "\n",
    "        X_train_list.append(X_train)\n",
    "        y_train_list.append(y_train)\n",
    "        X_val_list.append(X_val)\n",
    "        y_val_list.append(y_val)\n",
    "\n",
    "    return X_train_list, y_train_list, X_val_list, y_val_list, dates_list\n",
    "\n",
    "X_train_list, y_train_list, X_val_list, y_val_list, dates_list = k_fold_split(train_features, target, dates, 5)\n",
    "sets = [X_train_list, y_train_list, X_val_list, y_val_list, dates_list]\n",
    "\n",
    "for i in sets:\n",
    "    for x in i:\n",
    "        print (x.shape)\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "new_dates = pd.DataFrame(dates_list)\n",
    "new_dates['dates'] = new_dates[0].apply(lambda x: datetime.date(int(str(x)[:4]), int(str(x)[4:6]), int(str(x)[6:])))\n",
    "print (new_dates)\n",
    "\n",
    "dates_long = pd.DataFrame(dates)\n",
    "dates_long['date'] = dates_long['date'].apply(lambda x: datetime.date(int(str(x)[:4]), int(str(x)[4:6]), int(str(x)[6:])))\n",
    "dates_long.shape\n",
    "         \n",
    "def performance_metric(predict, true):\n",
    "    \n",
    "    mse = mean_squared_error(predict, true)\n",
    "    rmse = np.sqrt(mean_squared_error(predict, true))\n",
    "    return mse, rmse\n",
    "def train_predict(model, X_train, y_train, X_val, y_val):\n",
    "\n",
    "    model = model.fit(X_train, y_train)\n",
    "\n",
    "    val_predictions = model.predict(X_val)\n",
    "\n",
    "    mse, rmse = performance_metric(val_predictions, y_val)\n",
    "\n",
    "    return mse, rmse\n",
    "\n",
    "    print('Validation MSE: %.2f' % mse)\n",
    "    print('Validation RMSE: %.2f' % rmse)\n",
    "\n",
    "model = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "MSE = []\n",
    "RMSE = []\n",
    "\n",
    "for i in range(len(X_train_list)):\n",
    "    X_train = X_train_list[i]\n",
    "    y_train = y_train_list[i]\n",
    "    X_val = X_val_list[i]\n",
    "    y_val = y_val_list[i]\n",
    "\n",
    "    mse, rmse = train_predict(model, X_train, y_train, X_val, y_val)\n",
    "    MSE.append(mse)\n",
    "    RMSE.append(rmse)\n",
    "plt.plot(new_dates['dates'], MSE, 'go--', linewidth=2, markersize=2)\n",
    "plt.plot(new_dates['dates'], RMSE, 'bo-', linewidth=2, markersize=2)\n",
    "plt.legend(('MSE', 'RMSE'),\n",
    "        loc=(1, 1), handlelength=1.5, fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "predictions = model.predict(np.array(test))\n",
    "\n",
    "submission = pd.DataFrame({\"fullVisitorId\":test_ids})\n",
    "predictions[predictions<0] = 0\n",
    "submission[\"PredictedLogRevenue\"] = predictions\n",
    "submission = submission.groupby(\"fullVisitorId\")[\"PredictedLogRevenue\"].sum().reset_index()\n",
    "submission.columns = [\"fullVisitorId\", \"PredictedLogRevenue\"]\n",
    "submission[\"PredictedLogRevenue\"] = submission[\"PredictedLogRevenue\"]\n",
    "submission.to_csv(\"data/submission.csv\", index=False)\n",
    "print (submission.shape)\n",
    "submission.head(10)\n",
    "\n",
    "def shap_plot(model, X_val):\n",
    "\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_val)\n",
    "\n",
    "    shap.summary_plot(shap_values, X_val)\n",
    "i = -1\n",
    "X_train = X_train_list[i]\n",
    "y_train = y_train_list[i]\n",
    "X_val = X_val_list[i]\n",
    "y_val = y_val_list[i]\n",
    "model = DecisionTreeRegressor(random_state=42).fit(X_train, y_train)\n",
    "shap_plot(model, X_val)\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) is a unified approach to explain the output of any machine learning model. (https://github.com/slundberg/shap)\n",
    "\n",
    "high pageviews contribute to a higher totalRevenue according to the SHAP plot\n",
    "def feature_importance(model, X, y):\n",
    "    perm = PermutationImportance(model, random_state=42).fit(X, y)\n",
    "    return eli5.show_weights(perm, feature_names = X.columns.tolist())\n",
    "feature_importance(model, X_val, y_val)\n",
    "\n",
    "def plot_metrics(loss, val_loss):\n",
    "    fig, (ax1) = plt.subplots(1, 1, sharex='col', figsize=(20,7))\n",
    "    ax1.plot(loss, label='Train loss')\n",
    "    ax1.plot(val_loss, label='Validation loss')\n",
    "    ax1.legend(loc='best')\n",
    "    ax1.set_title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.show()\n",
    "def plot_diff(X, diff):\n",
    "    fig = plt.subplots(figsize=(18,6))\n",
    "    plt.plot(diff, linewidth=1)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "BATCH_SIZE = 64 # decrease if memory is not large enough\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 1e-2\n",
    "\n",
    "# Sandbox model creation\n",
    "model = Sequential()\n",
    "model.add(Dense(426, kernel_initializer='glorot_normal', input_dim=X_train_list[0].shape[1]))\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(214, kernel_initializer='glorot_normal'))\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(108, kernel_initializer='glorot_normal'))\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(54, kernel_initializer='glorot_normal'))\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "model.add(Dense(1))\n",
    "Final model setup\n",
    "model = Sequential()\n",
    "model.add(Dense(256, kernel_initializer='glorot_normal', activation='relu', input_dim=X_train_list[0].shape[1]))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(128, kernel_initializer='glorot_normal', activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, kernel_initializer='glorot_normal', activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(16, kernel_initializer='glorot_normal', activation='relu'))\n",
    "model.add(Dense(1))\n",
    "# Setting a learning rate decay through the scheduler callback\n",
    "# To be used in place of the decay rate in the optimizer\n",
    "def schedule(epoch):\n",
    "    print (LEARNING_RATE*(1/(epoch+1)))\n",
    "    return LEARNING_RATE*(1/(epoch+1))\n",
    "\n",
    "adam = optimizers.adam(lr=LEARNING_RATE, decay=0.001)\n",
    "model.compile(loss='mse', optimizer=adam)\n",
    "\n",
    "callback = [\n",
    "  # Interrupt training if `val_loss` stops improving for so many epochs (patience) by an amount more than min_delta\n",
    "  EarlyStopping(patience=4, monitor='val_loss', min_delta=0.0005),\n",
    "  # Write TensorBoard logs to `./logs` directory\n",
    "  TensorBoard(log_dir='./logs'),\n",
    "  # Define a learning rate decay\n",
    "#   LearningRateScheduler(schedule)\n",
    "]\n",
    "\n",
    "# To view Tensorboard \n",
    "# enter in command line  tensorboard --logdir='./logs/' and go to localhost:6006 in browser\n",
    "Create additional callbacks\n",
    "\n",
    "model.summary()\n",
    "\n",
    "x = [4,3,2]\n",
    "y = [1,1,1]\n",
    "difference = np.subtract(x, y)\n",
    "print (difference)\n",
    "[3 2 1]\n",
    "MSE = []\n",
    "RMSE = []\n",
    "\n",
    "for i in range(len(X_train_list)):\n",
    "    X_train = np.array(X_train_list[i])\n",
    "    y_train = np.array(y_train_list[i])\n",
    "    X_val = np.array(X_val_list[i])\n",
    "    y_val = np.array(y_val_list[i])\n",
    "\n",
    "    history = model.fit(x=X_train, y=y_train, batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "                    verbose=1, callbacks=callback, validation_data=(X_val, y_val))\n",
    "\n",
    "    val_predictions = model.predict(X_val)\n",
    "    mse, rmse = performance_metric(val_predictions, y_val)\n",
    "    MSE.append(mse)\n",
    "    RMSE.append(rmse)\n",
    "\n",
    "    difference = np.subtract(y_val, val_predictions)\n",
    "\n",
    "    print('Model validation metrics')\n",
    "    print ('Run {} of {}'.format(i+1, len(X_train_list)))\n",
    "    print('MSE: %.2f' % mse)\n",
    "    print('RMSE: %.2f' % rmse)\n",
    "    plot_metrics(history.history['loss'], history.history['val_loss'])\n",
    "    plot_diff(X_train, difference)\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(new_dates['dates'], MSE, 'go--', linewidth=2, markersize=2)\n",
    "plt.plot(new_dates['dates'], RMSE, 'bo-', linewidth=2, markersize=2)\n",
    "plt.legend(('MSE', 'RMSE'),\n",
    "        loc=(1, 1), handlelength=1.5, fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "predictions = model.predict(np.array(test))\n",
    "\n",
    "submission = pd.DataFrame({\"fullVisitorId\":test_ids})\n",
    "# submission = submission[:20000]\n",
    "predictions[predictions<0] = 0\n",
    "submission[\"PredictedLogRevenue\"] = predictions\n",
    "submission = submission.groupby(\"fullVisitorId\")[\"PredictedLogRevenue\"].sum().reset_index()\n",
    "submission.columns = [\"fullVisitorId\", \"PredictedLogRevenue\"]\n",
    "submission[\"PredictedLogRevenue\"] = submission[\"PredictedLogRevenue\"]\n",
    "submission.to_csv(\"data/submission.csv\", index=False)\n",
    "Shape of submission should be (617242, 2)\n",
    "\n",
    "print (submission.shape)\n",
    "submission.head(10)\n",
    "\n",
    "model.save('models/model_11.h5')\n",
    "\n",
    "model = models.load_model('models/model_9.h5')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
